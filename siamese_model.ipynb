{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2ae44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0-rc0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary library files\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.keras.layers import Input, Activation, GlobalAveragePooling2D, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3da80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set common params\n",
    "im_height = 200\n",
    "im_width = 200\n",
    "num_channels = 3\n",
    "num_p = 20\n",
    "num_im = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bad1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_element(exclude, range_):\n",
    "    return random.choice(list(set([x for x in range(0, range_)]) - set([exclude])))\n",
    "\n",
    "def generate_pairs(vid):\n",
    "    \n",
    "#     y1, y2, y3, y4\n",
    "#     s1, s2, s3, s4\n",
    "#     y1, yr 1\n",
    "#     y1, sr 0\n",
    "#     y2, yr 1\n",
    "#     y2, sr 0\n",
    "#     y3, yr\n",
    "#     y3, sr\n",
    "#     y4, yr 1\n",
    "#     y4, sr, 0\n",
    "\n",
    "#     2,4\n",
    "#     im[0][2] -> 0th person, 2nd image\n",
    "#     im[1][1] -> \n",
    "\n",
    "#     75*20*100*2\n",
    "    images = np.zeros((num_p, num_im, im_height, im_width, num_channels))\n",
    "    path = 'dataset/jpg_Extracted_PIDS/'+str(vid)+'/'\n",
    "    folders = ([name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))])\n",
    "    folders.sort() # People\n",
    "    folders = folders[1:1+num_p]\n",
    "    for i,folder in enumerate(folders):\n",
    "        temp_path = path+folder+'/'\n",
    "        files = glob.glob(temp_path+'*', recursive=True) # Image\n",
    "        files = random.choices(files, k=num_im)\n",
    "        for j,filename in enumerate(files):\n",
    "            with Image.open(filename) as im:\n",
    "                im = im.resize((im_height, im_width))\n",
    "                images[i,j] = np.array(im)\n",
    "                \n",
    "#     plt.imshow(images[1,0].astype(\"uint8\"))\n",
    "    X = np.zeros((num_p*num_im*2, 2, im_height, im_width, num_channels)) # (16 X 2 X 200 X 200 X 3)\n",
    "    Y = np.zeros((num_p*num_im*2))\n",
    "    v = 0\n",
    "    for i in range(num_p):\n",
    "        for j in range(num_im):\n",
    "            p1 = np.concatenate(([images[i][j]],[images[i][random_element(j,num_im)]]))\n",
    "            p2 = np.concatenate(([images[i][j]],[images[random_element(i,num_p)][random_element(j,num_im)]]))\n",
    "            X[v] = p1\n",
    "            Y[v] = 1.0\n",
    "            v+=1\n",
    "            X[v] = p2\n",
    "            Y[v] = 0.0\n",
    "            v+=1\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be11bcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2, 200, 200, 3) (4000,)\n"
     ]
    }
   ],
   "source": [
    "X, Y = generate_pairs('12-11-2019-4-1')\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826e51ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos: 75\n"
     ]
    }
   ],
   "source": [
    "path = 'dataset/jpg_Extracted_PIDS/'\n",
    "folders = ([name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))])\n",
    "# print(folders)\n",
    "print(\"Number of videos:\", len(folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9754acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with a generator\n",
    "def data_generator(batch_size, eph):\n",
    "    for _ in range(eph):\n",
    "#         steps = 300000//batch_size\n",
    "        # our steps will be 3000\n",
    "        for f in folders:\n",
    "            X_batch, Y_batch = generate_pairs(f) # 4000\n",
    "            for i in  range(0, num_p*num_im*2, batch_size):\n",
    "                yield [X_batch[i:i+batch_size, 0], X_batch[i:i+batch_size, 1]], Y_batch[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d3543d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.InceptionV3(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(im_height, im_width, num_channels),\n",
    "    include_top=False,\n",
    ")\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1e9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_distance(inputs):\n",
    "    input1, input2 = inputs\n",
    "    output = K.abs(input1 - input2)\n",
    "    return output\n",
    "\n",
    "def l1_distance_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    assert shape1 == shape2\n",
    "    return (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe68c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model():\n",
    "    input1 = Input(shape=(im_height, im_width, num_channels,))\n",
    "    input2 = Input(shape=(im_height, im_width, num_channels,))\n",
    "    processed_input1 = tf.keras.applications.inception_v3.preprocess_input(input1)\n",
    "    processed_input2 = tf.keras.applications.inception_v3.preprocess_input(input2)\n",
    "    feature_vec1 = base_model(processed_input1)\n",
    "    feature_vec2 = base_model(processed_input2)\n",
    "    pool1 = GlobalAveragePooling2D()(feature_vec1) # 6x6x2048 -> 2048\n",
    "    pool2 = GlobalAveragePooling2D()(feature_vec2)\n",
    "    distance = Lambda(l1_distance, output_shape=l1_distance_output_shape)([pool1, pool2])\n",
    "#     pool = GlobalAveragePooling2D()(distance)\n",
    "    d1 = Dense(256)(distance)\n",
    "    d2 = Dense(1)(d1)\n",
    "    output = Activation('sigmoid')(d2)\n",
    "    return Model(inputs=[input1, input2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7933ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 200, 200, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 200, 200, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (None, 200, 200, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_1 (TFOpLambda)  (None, 200, 200, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 200, 200, 3)  0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_1 (TFOpLambda) (None, 200, 200, 3)  0           tf.math.truediv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_v3 (Functional)       (None, 4, 4, 2048)   21802784    tf.math.subtract[0][0]           \n",
      "                                                                 tf.math.subtract_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           inception_v3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           inception_v3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 2048)         0           global_average_pooling2d[0][0]   \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 1)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 22,327,585\n",
      "Trainable params: 524,801\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = siamese_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d19f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x29f16a0d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x29f16a0d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 12:51:04.955307: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-02-25 12:51:04.955514: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/3000 [..............................] - ETA: 19:12:38 - loss: 2.5792 - binary_accuracy: 0.4918"
     ]
    }
   ],
   "source": [
    "bs = 100\n",
    "eph = 5\n",
    "model.fit(data_generator(bs, eph), batch_size=bs, epochs=eph, steps_per_epoch=3000)\n",
    "# model.fit([X[:,0], X[:,1]], Y, batch_size=bs, epochs=eph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaf39d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "400/400 [==============================] - 176s 437ms/step - loss: 0.5913 - binary_accuracy: 0.6780\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 191s 477ms/step - loss: 0.5821 - binary_accuracy: 0.6865\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 198s 495ms/step - loss: 0.5750 - binary_accuracy: 0.6945\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 216s 540ms/step - loss: 0.5757 - binary_accuracy: 0.6960\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 203s 506ms/step - loss: 0.5653 - binary_accuracy: 0.7035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x171fbf5e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.trainable = True\n",
    "bs = 10\n",
    "eph = 5\n",
    "model.fit(data_generator(bs, eph), epochs=eph, steps_per_epoch=3000)\n",
    "# model.fit([X[:,0], X[:,1]], Y, epochs=eph, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ced710dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('siamese_1vid.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e99589",
   "metadata": {},
   "source": [
    "References:\n",
    "    https://github.com/aup8497/Person-Re-identification-using-Siamese-networks/blob/master/Code/siamese_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b98c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c2f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_apple_mlc_enabled True\n",
      "is_tf_compiled_with_apple_mlc True\n",
      "eagerly? False\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "### Aman's code to enable the GPU\n",
    "from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "mlcompute.set_mlc_device(device_name='gpu')\n",
    "print(\"is_apple_mlc_enabled %s\" % mlcompute.is_apple_mlc_enabled())\n",
    "print(\"is_tf_compiled_with_apple_mlc %s\" % mlcompute.is_tf_compiled_with_apple_mlc())\n",
    "print(f\"eagerly? {tf.executing_eagerly()}\")\n",
    "print(tf.config.list_logical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615ad31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
